{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c328512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "# For automatic dataset downloading\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    from BytesIO import BytesIO\n",
    "except ImportError:\n",
    "    from io import BytesIO\n",
    "\n",
    "\n",
    "def data_iterator(data, batch_size):\n",
    "    \"\"\"\n",
    "    A simple data iterator from https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "    :param data: list of numpy tensors that need to be randomly batched across their first dimension.\n",
    "    :param batch_size: int, batch_size of data_iterator.\n",
    "    Assumes same first dimension size of all numpy tensors.\n",
    "    :return: iterator over batches of numpy tensors\n",
    "    \"\"\"\n",
    "    # shuffle labels and features\n",
    "    max_idx = len(data[0])\n",
    "    idxs = np.arange(0, max_idx)\n",
    "    np.random.shuffle(idxs)\n",
    "    shuf_data = [dat[idxs] for dat in data]\n",
    "\n",
    "    # Does not yield last remainder of size less than batch_size\n",
    "    for i in range(max_idx//batch_size):\n",
    "        data_batch = [dat[i*batch_size:(i+1)*batch_size] for dat in shuf_data]\n",
    "        yield data_batch\n",
    "\n",
    "\n",
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array([id_dict[x] for x in data])\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n\n",
    "\n",
    "\n",
    "def download_dataset(dataset, files, data_dir):\n",
    "    \"\"\" Downloads dataset if files are not present. \"\"\"\n",
    "\n",
    "    if not np.all([os.path.isfile(data_dir + f) for f in files]):\n",
    "        url = \"http://files.grouplens.org/datasets/movielens/\" + dataset.replace('_', '-') + '.zip'\n",
    "        request = urlopen(url)\n",
    "\n",
    "        print('Downloading %s dataset' % dataset)\n",
    "\n",
    "        if dataset in ['ml_100k', 'ml_1m']:\n",
    "            target_dir = 'raw_data/' + dataset.replace('_', '-')\n",
    "        elif dataset == 'ml_10m':\n",
    "            target_dir = 'raw_data/' + 'ml-10M100K'\n",
    "        else:\n",
    "            raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "        with ZipFile(BytesIO(request.read())) as zip_ref:\n",
    "            zip_ref.extractall('raw_data/')\n",
    "\n",
    "        os.rename(target_dir, data_dir)\n",
    "        #shutil.rmtree(target_dir)\n",
    "def load_data(fname, seed=1234, verbose=True):\n",
    "    \"\"\" Loads dataset and creates adjacency matrix\n",
    "    and feature matrix\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str, dataset\n",
    "    seed: int, dataset shuffling seed\n",
    "    verbose: to print out statements or not\n",
    "    Returns\n",
    "    -------\n",
    "    num_users : int\n",
    "        Number of users and items respectively\n",
    "    num_items : int\n",
    "    u_nodes : np.int32 arrays\n",
    "        User indices\n",
    "    v_nodes : np.int32 array\n",
    "        item (movie) indices\n",
    "    ratings : np.float32 array\n",
    "        User/item ratings s.t. ratings[k] is the rating given by user u_nodes[k] to\n",
    "        item v_nodes[k]. Note that that the all pairs u_nodes[k]/v_nodes[k] are unique, but\n",
    "        not necessarily all u_nodes[k] or all v_nodes[k] separately.\n",
    "    u_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "    v_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "    seed: int,\n",
    "        For datashuffling seed with pythons own random.shuffle, as in CF-NADE.\n",
    "    \"\"\"\n",
    "\n",
    "    u_features = None\n",
    "    v_features = None\n",
    "\n",
    "    print('Loading dataset', fname)\n",
    "\n",
    "    data_dir = 'raw_data/' + fname\n",
    "\n",
    "    if fname == 'ml_100k':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/u.data', '/u.item', '/u.user']\n",
    "\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = '\\t'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        data = pd.read_csv(\n",
    "            filename, sep=sep, header=None,\n",
    "            names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.values.tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "        ratings = ratings.astype(np.float64)\n",
    "\n",
    "        # Movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = data_dir + files[1]\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # User features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age']\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    elif fname == 'ml_1m':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/ratings.dat', '/movies.dat', '/users.dat']\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = r'\\:\\:'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "        data = pd.read_csv(filename, sep=sep, header=None,\n",
    "                           names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python', encoding='latin-1')\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.values.tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "        # Load movie features\n",
    "        movies_file = data_dir + files[1]\n",
    "\n",
    "        movies_headers = ['movie_id', 'title', 'genre']\n",
    "        movies_df = pd.read_csv(movies_file, sep=sep, header=None,\n",
    "                                names=movies_headers, engine='python', encoding='latin-1')\n",
    "\n",
    "        # Extracting all genres\n",
    "        genres = []\n",
    "        for s in movies_df['genre'].values:\n",
    "            genres.extend(s.split('|'))\n",
    "\n",
    "        genres = list(set(genres))\n",
    "        num_genres = len(genres)\n",
    "\n",
    "        genres_dict = {g: idx for idx, g in enumerate(genres)}\n",
    "\n",
    "        # Creating 0 or 1 valued features for all genres\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, s in zip(movies_df['movie_id'].values.tolist(), movies_df['genre'].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                gen = s.split('|')\n",
    "                for g in gen:\n",
    "                    v_features[v_dict[movie_id], genres_dict[g]] = 1.\n",
    "\n",
    "        # Load user features\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user_id', 'gender', 'age', 'occupation', 'zip-code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python',encoding='latin-1')\n",
    "\n",
    "        # Extracting all features\n",
    "        cols = users_df.columns.values[1:]\n",
    "\n",
    "        cntr = 0\n",
    "        feat_dicts = []\n",
    "        for header in cols:\n",
    "            d = dict()\n",
    "            feats = np.unique(users_df[header].values).tolist()\n",
    "            d.update({f: i for i, f in enumerate(feats, start=cntr)})\n",
    "            feat_dicts.append(d)\n",
    "            cntr += len(d)\n",
    "\n",
    "        num_feats = sum(len(d) for d in feat_dicts)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user_id']\n",
    "            if u_id in u_dict.keys():\n",
    "                for k, header in enumerate(cols):\n",
    "                    u_features[u_dict[u_id], feat_dicts[k][row[header]]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    elif fname == 'ml_10m':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/ratings.dat']\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = r'\\:\\:'\n",
    "\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "        data = pd.read_csv(filename, sep=sep, header=None,\n",
    "                           names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python')\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.values.tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "    elif fname == 'ml_25m':\n",
    "\n",
    "        # Please download the processed movielens25M.csv to raw_data/ml_25m/\n",
    "        # Each row is uid,iid,cid,time,rating, sorted by time\n",
    "        files = ['/movielens25M.csv']\n",
    "        row_count = 24999850\n",
    "\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        chunksize = 10000\n",
    "        data = pd.DataFrame()\n",
    "        pbar = tqdm(pd.read_csv(filename, header=0, usecols=['uid', 'iid', 'rating'], \n",
    "                    chunksize=chunksize), total=row_count//chunksize)\n",
    "        for chunk in pbar:\n",
    "            data = pd.concat([data, chunk], ignore_index=True)\n",
    "\n",
    "        data_array = data.values\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0]\n",
    "        v_nodes_ratings = data_array[:, 1]\n",
    "        ratings = data_array[:, 2]\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Dataset name not recognized: ' + fname)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of users = %d' % num_users)\n",
    "        print('Number of items = %d' % num_items)\n",
    "        print('Number of links = %d' % ratings.shape[0])\n",
    "        print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    return num_users, num_items, u_nodes_ratings, v_nodes_ratings, ratings, u_features, v_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b939d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ml_1m\n",
      "Number of users = 6040\n",
      "Number of items = 3706\n",
      "Number of links = 1000209\n",
      "Fraction of positive links = 0.0447\n"
     ]
    }
   ],
   "source": [
    "num_users, num_items, u_nodes_ratings, v_nodes_ratings, ratings, u_features, v_features = load_data('ml_1m', seed=1234, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa524546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6edab953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be7dd60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6039"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(u_nodes_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8b1f3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 162, 3926, 3266, ...,  790, 2849, 4897], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_nodes_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9c47d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e961a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GAN_ROBERTA] *",
   "language": "python",
   "name": "conda-env-GAN_ROBERTA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
